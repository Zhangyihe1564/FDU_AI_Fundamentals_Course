{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch是什么?\n",
    "================\n",
    "\n",
    "基于Python的科学计算包，服务于以下两种场景:\n",
    "\n",
    "-  作为NumPy的替代品，可以使用GPU的强大计算能力\n",
    "-  提供最大的灵活性和高速的深度学习研究平台"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:37.472874Z",
     "iopub.status.busy": "2025-02-24T04:04:37.472708Z",
     "iopub.status.idle": "2025-02-24T04:04:40.181053Z",
     "shell.execute_reply": "2025-02-24T04:04:40.180643Z",
     "shell.execute_reply.started": "2025-02-24T04:04:37.472858Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先要引入相关的包\n",
    "import torch\n",
    "import numpy as np\n",
    "#打印一下版本\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 张量(Tensor)\n",
    "张量的英文是Tensor，它是PyTorch里面基础的运算单位，与Numpy的ndarray相同都表示的是一个多维的矩阵。\n",
    "与ndarray的最大区别就是，PyTorch的Tensor可以在 GPU 上运行，而 numpy 的 ndarray 只能在 CPU 上运行，在GPU上运行大大加快了运算速度。\n",
    "\n",
    "下面我们生成一个简单的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:45.862869Z",
     "iopub.status.busy": "2025-02-24T04:04:45.862486Z",
     "iopub.status.idle": "2025-02-24T04:04:45.928563Z",
     "shell.execute_reply": "2025-02-24T04:04:45.928124Z",
     "shell.execute_reply.started": "2025-02-24T04:04:45.862846Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4862, 0.0959, 0.4101],\n",
       "        [0.8837, 0.6365, 0.2752]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上生成了一个，2行3列的的矩阵，我们看一下他的大小："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:47.472097Z",
     "iopub.status.busy": "2025-02-24T04:04:47.471798Z",
     "iopub.status.idle": "2025-02-24T04:04:47.475371Z",
     "shell.execute_reply": "2025-02-24T04:04:47.474741Z",
     "shell.execute_reply.started": "2025-02-24T04:04:47.472079Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 可以使用与numpy相同的shape属性查看\n",
    "print(x.shape)\n",
    "# 也可以使用size()函数，返回的结果都是相同的\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量（Tensor）是一个定义在一些向量空间和一些对偶空间的笛卡儿积上的多重线性映射，其坐标是|n|维空间内，有|n|个分量的一种量， 其中每个分量都是坐标的函数， 而在坐标变换时，这些分量也依照某些规则作线性变换。r称为该张量的秩或阶（与矩阵的秩和阶均无关系）。 (来自百度百科)\n",
    "\n",
    "下面我们来生成一些多维的张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:48.450509Z",
     "iopub.status.busy": "2025-02-24T04:04:48.450261Z",
     "iopub.status.idle": "2025-02-24T04:04:48.455378Z",
     "shell.execute_reply": "2025-02-24T04:04:48.455009Z",
     "shell.execute_reply.started": "2025-02-24T04:04:48.450493Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5562, 0.3842, 0.2514, 0.4940, 0.5484],\n",
       "          [0.9414, 0.5889, 0.1684, 0.3432, 0.7726],\n",
       "          [0.6179, 0.3233, 0.8470, 0.2098, 0.6997],\n",
       "          [0.8537, 0.1483, 0.5064, 0.0885, 0.7669]],\n",
       "\n",
       "         [[0.2402, 0.2539, 0.8411, 0.2927, 0.9911],\n",
       "          [0.0589, 0.8525, 0.1928, 0.4497, 0.6297],\n",
       "          [0.6905, 0.4514, 0.2092, 0.3016, 0.4565],\n",
       "          [0.5864, 0.7645, 0.8926, 0.2744, 0.3461]],\n",
       "\n",
       "         [[0.3078, 0.5705, 0.9848, 0.9559, 0.1580],\n",
       "          [0.0366, 0.0463, 0.8166, 0.1174, 0.4081],\n",
       "          [0.8139, 0.3438, 0.8362, 0.1651, 0.9217],\n",
       "          [0.6922, 0.9159, 0.0387, 0.6195, 0.3690]]],\n",
       "\n",
       "\n",
       "        [[[0.7739, 0.8689, 0.6118, 0.5030, 0.1638],\n",
       "          [0.5203, 0.1606, 0.2875, 0.5151, 0.1226],\n",
       "          [0.1190, 0.6046, 0.7995, 0.0946, 0.9164],\n",
       "          [0.9598, 0.6774, 0.1394, 0.5881, 0.7331]],\n",
       "\n",
       "         [[0.0325, 0.1449, 0.6996, 0.9127, 0.1206],\n",
       "          [0.3737, 0.2166, 0.4211, 0.2164, 0.3164],\n",
       "          [0.8113, 0.7959, 0.1772, 0.6531, 0.8943],\n",
       "          [0.5132, 0.3081, 0.9010, 0.7745, 0.8688]],\n",
       "\n",
       "         [[0.4349, 0.2466, 0.6368, 0.2711, 0.8356],\n",
       "          [0.5986, 0.3639, 0.7062, 0.7554, 0.8343],\n",
       "          [0.2649, 0.8422, 0.6238, 0.6818, 0.6832],\n",
       "          [0.2535, 0.3314, 0.8942, 0.5965, 0.9532]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.rand(2,3,4,5)\n",
    "print(y.size())\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在同构的意义下，第零阶张量 （r = 0） 为标量 （Scalar），第一阶张量 （r = 1） 为向量 （Vector）， 第二阶张量 （r = 2） 则成为矩阵 （Matrix），第三阶以上的统称为多维张量。\n",
    "\n",
    "其中要特别注意的就是标量，我们先生成一个标量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:49.050751Z",
     "iopub.status.busy": "2025-02-24T04:04:49.050375Z",
     "iopub.status.idle": "2025-02-24T04:04:49.054313Z",
     "shell.execute_reply": "2025-02-24T04:04:49.053949Z",
     "shell.execute_reply.started": "2025-02-24T04:04:49.050734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1433)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们直接使用现有数字生成\n",
    "scalar =torch.tensor(3.1433223)\n",
    "print(scalar)\n",
    "#打印标量的大小\n",
    "scalar.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于标量，我们可以直接使用 .item() 从中取出其对应的python对象的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:49.465615Z",
     "iopub.status.busy": "2025-02-24T04:04:49.465404Z",
     "iopub.status.idle": "2025-02-24T04:04:49.468408Z",
     "shell.execute_reply": "2025-02-24T04:04:49.467945Z",
     "shell.execute_reply.started": "2025-02-24T04:04:49.465600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.143322229385376"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特别的：如果张量中只有一个元素的tensor也可以调用`tensor.item`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:49.898404Z",
     "iopub.status.busy": "2025-02-24T04:04:49.898145Z",
     "iopub.status.idle": "2025-02-24T04:04:49.902649Z",
     "shell.execute_reply": "2025-02-24T04:04:49.902141Z",
     "shell.execute_reply.started": "2025-02-24T04:04:49.898388Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1433])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([3.1433223]) \n",
    "print(tensor)\n",
    "tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:50.099381Z",
     "iopub.status.busy": "2025-02-24T04:04:50.099033Z",
     "iopub.status.idle": "2025-02-24T04:04:50.102466Z",
     "shell.execute_reply": "2025-02-24T04:04:50.101957Z",
     "shell.execute_reply.started": "2025-02-24T04:04:50.099367Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.143322229385376"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本类型\n",
    "Tensor的基本数据类型有五种：\n",
    "- 32位浮点型：torch.FloatTensor。 (默认)\n",
    "- 64位整型：torch.LongTensor。\n",
    "- 32位整型：torch.IntTensor。\n",
    "- 16位整型：torch.ShortTensor。\n",
    "- 64位浮点型：torch.DoubleTensor。\n",
    "\n",
    "除以上数字类型外，还有\n",
    "byte和chart型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:50.501082Z",
     "iopub.status.busy": "2025-02-24T04:04:50.500794Z",
     "iopub.status.idle": "2025-02-24T04:04:50.504200Z",
     "shell.execute_reply": "2025-02-24T04:04:50.503843Z",
     "shell.execute_reply.started": "2025-02-24T04:04:50.501067Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long=tensor.long()\n",
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:50.808665Z",
     "iopub.status.busy": "2025-02-24T04:04:50.808308Z",
     "iopub.status.idle": "2025-02-24T04:04:50.814699Z",
     "shell.execute_reply": "2025-02-24T04:04:50.814342Z",
     "shell.execute_reply.started": "2025-02-24T04:04:50.808650Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1426], dtype=torch.float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half=tensor.half()\n",
    "half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:51.057149Z",
     "iopub.status.busy": "2025-02-24T04:04:51.056740Z",
     "iopub.status.idle": "2025-02-24T04:04:51.060111Z",
     "shell.execute_reply": "2025-02-24T04:04:51.059709Z",
     "shell.execute_reply.started": "2025-02-24T04:04:51.057132Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_t=tensor.int()\n",
    "int_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:51.217676Z",
     "iopub.status.busy": "2025-02-24T04:04:51.217384Z",
     "iopub.status.idle": "2025-02-24T04:04:51.221094Z",
     "shell.execute_reply": "2025-02-24T04:04:51.220591Z",
     "shell.execute_reply.started": "2025-02-24T04:04:51.217661Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1433])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flo = tensor.float()\n",
    "flo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:51.400317Z",
     "iopub.status.busy": "2025-02-24T04:04:51.400111Z",
     "iopub.status.idle": "2025-02-24T04:04:51.403766Z",
     "shell.execute_reply": "2025-02-24T04:04:51.403221Z",
     "shell.execute_reply.started": "2025-02-24T04:04:51.400302Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], dtype=torch.int16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = tensor.short()\n",
    "short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:52.082680Z",
     "iopub.status.busy": "2025-02-24T04:04:52.082391Z",
     "iopub.status.idle": "2025-02-24T04:04:52.086340Z",
     "shell.execute_reply": "2025-02-24T04:04:52.085954Z",
     "shell.execute_reply.started": "2025-02-24T04:04:52.082665Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], dtype=torch.int8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = tensor.char()\n",
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:52.545312Z",
     "iopub.status.busy": "2025-02-24T04:04:52.545023Z",
     "iopub.status.idle": "2025-02-24T04:04:52.548564Z",
     "shell.execute_reply": "2025-02-24T04:04:52.548062Z",
     "shell.execute_reply.started": "2025-02-24T04:04:52.545297Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], dtype=torch.uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt = tensor.byte()\n",
    "bt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy转换\n",
    "使用numpy方法将Tensor转为ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:54.084788Z",
     "iopub.status.busy": "2025-02-24T04:04:54.084323Z",
     "iopub.status.idle": "2025-02-24T04:04:54.089100Z",
     "shell.execute_reply": "2025-02-24T04:04:54.088671Z",
     "shell.execute_reply.started": "2025-02-24T04:04:54.084769Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0321832   0.622237  ]\n",
      " [ 0.0309437  -0.32825637]\n",
      " [ 0.8479681   1.8006579 ]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((3, 2))\n",
    "# tensor转化为numpy\n",
    "numpy_a = a.numpy()\n",
    "print(numpy_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy转化为Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:54.996416Z",
     "iopub.status.busy": "2025-02-24T04:04:54.996130Z",
     "iopub.status.idle": "2025-02-24T04:04:55.000578Z",
     "shell.execute_reply": "2025-02-24T04:04:55.000052Z",
     "shell.execute_reply.started": "2025-02-24T04:04:54.996398Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0322,  0.6222],\n",
       "        [ 0.0309, -0.3283],\n",
       "        [ 0.8480,  1.8007]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_a = torch.from_numpy(numpy_a)\n",
    "torch_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设备间转换\n",
    "使用``.to`` 方法 可以将Tensor移动到任何设备中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:04:56.938091Z",
     "iopub.status.busy": "2025-02-24T04:04:56.937787Z",
     "iopub.status.idle": "2025-02-24T04:04:56.941291Z",
     "shell.execute_reply": "2025-02-24T04:04:56.940909Z",
     "shell.execute_reply.started": "2025-02-24T04:04:56.938074Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_a=torch.rand(4, 3)\n",
    "cpu_a.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用.to(\"cuda\")，可以将Tensor移动到GPU设备中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:01.058501Z",
     "iopub.status.busy": "2025-02-24T04:05:01.058177Z",
     "iopub.status.idle": "2025-02-24T04:05:04.236399Z",
     "shell.execute_reply": "2025-02-24T04:05:04.235990Z",
     "shell.execute_reply.started": "2025-02-24T04:05:01.058481Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "gpu_a=cpu_a.to(\"cuda\")\n",
    "gpu_a.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用.to(\"mps\")，可以将Tensor移动到M系列芯片MAC的加速计算框架上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.mps.FloatTensor'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.mps.is_available())\n",
    "gpu_a=cpu_a.to(\"mps\")\n",
    "gpu_a.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用.cpu方法将tensor移动到cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:08.702607Z",
     "iopub.status.busy": "2025-02-24T04:05:08.702305Z",
     "iopub.status.idle": "2025-02-24T04:05:08.706450Z",
     "shell.execute_reply": "2025-02-24T04:05:08.705865Z",
     "shell.execute_reply.started": "2025-02-24T04:05:08.702587Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_b=gpu_a.cpu()\n",
    "cpu_b.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``注意``：\n",
    "所有的 Tensor 类型默认都是基于CPU， CharTensor 类型不支持到\n",
    "NumPy 的转换.\n",
    "CUDA 张量\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化\n",
    "Pytorch中有许多默认的初始化方法可以使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:12.188395Z",
     "iopub.status.busy": "2025-02-24T04:05:12.188091Z",
     "iopub.status.idle": "2025-02-24T04:05:12.192797Z",
     "shell.execute_reply": "2025-02-24T04:05:12.192299Z",
     "shell.execute_reply.started": "2025-02-24T04:05:12.188376Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6064, 0.0511, 0.9958],\n",
       "        [0.4041, 0.8075, 0.5598],\n",
       "        [0.5235, 0.5278, 0.8778],\n",
       "        [0.6740, 0.3439, 0.2831],\n",
       "        [0.6060, 0.2081, 0.4131]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用[0,1]均匀分布随机初始化二维数组\n",
    "rnd = torch.rand(5, 3)\n",
    "rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:12.892437Z",
     "iopub.status.busy": "2025-02-24T04:05:12.892193Z",
     "iopub.status.idle": "2025-02-24T04:05:12.897388Z",
     "shell.execute_reply": "2025-02-24T04:05:12.896964Z",
     "shell.execute_reply.started": "2025-02-24T04:05:12.892419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##初始化，使用1填充\n",
    "one = torch.ones(2, 2)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:13.150950Z",
     "iopub.status.busy": "2025-02-24T04:05:13.150737Z",
     "iopub.status.idle": "2025-02-24T04:05:13.155043Z",
     "shell.execute_reply": "2025-02-24T04:05:13.154563Z",
     "shell.execute_reply.started": "2025-02-24T04:05:13.150933Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##初始化，使用0填充\n",
    "zero=torch.zeros(2,2)\n",
    "zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:13.380439Z",
     "iopub.status.busy": "2025-02-24T04:05:13.380017Z",
     "iopub.status.idle": "2025-02-24T04:05:13.387635Z",
     "shell.execute_reply": "2025-02-24T04:05:13.387237Z",
     "shell.execute_reply.started": "2025-02-24T04:05:13.380413Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始化一个单位矩阵，即对角线为1 其他为0\n",
    "eye=torch.eye(2,2)\n",
    "eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用方法\n",
    "PyTorch中对张量的操作api 和 NumPy 非常相似，如果熟悉 NumPy 中的操作，那么他们二者基本是一致的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:13.832181Z",
     "iopub.status.busy": "2025-02-24T04:05:13.831743Z",
     "iopub.status.idle": "2025-02-24T04:05:13.835436Z",
     "shell.execute_reply": "2025-02-24T04:05:13.834903Z",
     "shell.execute_reply.started": "2025-02-24T04:05:13.832161Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4541, -1.5342, -1.5037],\n",
      "        [ 0.6858, -0.8828,  0.1743],\n",
      "        [ 0.0505, -1.1970, -1.3877]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:14.057419Z",
     "iopub.status.busy": "2025-02-24T04:05:14.057251Z",
     "iopub.status.idle": "2025-02-24T04:05:14.064662Z",
     "shell.execute_reply": "2025-02-24T04:05:14.064183Z",
     "shell.execute_reply.started": "2025-02-24T04:05:14.057403Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4541, 0.6858, 0.0505]) tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# 沿着行取最大值\n",
    "max_value, max_idx = torch.max(x, dim=1)\n",
    "print(max_value, max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:14.316675Z",
     "iopub.status.busy": "2025-02-24T04:05:14.316375Z",
     "iopub.status.idle": "2025-02-24T04:05:14.320267Z",
     "shell.execute_reply": "2025-02-24T04:05:14.319762Z",
     "shell.execute_reply.started": "2025-02-24T04:05:14.316658Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.5838, -0.0226, -2.5341])\n"
     ]
    }
   ],
   "source": [
    "# 每行 x 求和\n",
    "sum_x = torch.sum(x, dim=1)\n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:14.544903Z",
     "iopub.status.busy": "2025-02-24T04:05:14.544637Z",
     "iopub.status.idle": "2025-02-24T04:05:14.549494Z",
     "shell.execute_reply": "2025-02-24T04:05:14.549073Z",
     "shell.execute_reply.started": "2025-02-24T04:05:14.544885Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6602, -3.2611, -1.9483],\n",
      "        [-0.3524, -1.4407, -0.4963],\n",
      "        [ 0.5742,  0.0713, -1.6183]])\n"
     ]
    }
   ],
   "source": [
    "y=torch.randn(3, 3)\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以_为结尾的，均会改变调用值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:14.900583Z",
     "iopub.status.busy": "2025-02-24T04:05:14.900346Z",
     "iopub.status.idle": "2025-02-24T04:05:14.904752Z",
     "shell.execute_reply": "2025-02-24T04:05:14.904185Z",
     "shell.execute_reply.started": "2025-02-24T04:05:14.900565Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6602, -3.2611, -1.9483],\n",
      "        [-0.3524, -1.4407, -0.4963],\n",
      "        [ 0.5742,  0.0713, -1.6183]])\n"
     ]
    }
   ],
   "source": [
    "# add 完成后x的值改变了\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的基本操作都介绍的的差不多了，下面介绍PyTorch的自动求导机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用PyTorch计算梯度数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch的Autograd模块实现了深度学习的算法中的向传播求导数，在张量（Tensor类）上的所有操作，Autograd都能为他们自动提供微分，简化了手动计算导数的复杂过程。\n",
    "\n",
    "在0.4以前的版本中，Pytorch 使用 Variable 类来自动计算所有的梯度。Variable类主要包含三个属性：\n",
    "data：保存Variable所包含的Tensor；grad：保存data对应的梯度，grad也是个Variable，而不是Tensor，它和data的形状一样；grad_fn：指向一个Function对象，这个Function用来反向传播计算输入的梯度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从0.4起， Variable 正式合并入Tensor类，通过Variable嵌套实现的自动微分功能已经整合进入了Tensor类中。虽然为了代码的兼容性还是可以使用Variable(tensor)这种方式进行嵌套，但是这个操作其实什么都没做。\n",
    "\n",
    "所以，以后的代码建议直接使用Tensor类进行操作，因为官方文档中已经将Variable设置成过期模块。\n",
    "\n",
    "要想通过Tensor类本身就支持了使用autograd功能，只需要设置.requires_grad=True\n",
    "\n",
    "Variable类中的的grad和grad_fn属性已经整合进入了Tensor类中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在张量创建时，通过设置 requires_grad 标识为Ture来告诉Pytorch需要对该张量进行自动求导，PyTorch会记录该张量的每一步操作历史并自动计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:17.031620Z",
     "iopub.status.busy": "2025-02-24T04:05:17.031329Z",
     "iopub.status.idle": "2025-02-24T04:05:17.036043Z",
     "shell.execute_reply": "2025-02-24T04:05:17.035515Z",
     "shell.execute_reply.started": "2025-02-24T04:05:17.031601Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4562, 0.5391, 0.5784, 0.9791, 0.5348],\n",
       "        [0.4466, 0.1700, 0.7257, 0.0305, 0.3195],\n",
       "        [0.8016, 0.9556, 0.4417, 0.6479, 0.1599],\n",
       "        [0.6445, 0.1676, 0.7844, 0.8477, 0.3878],\n",
       "        [0.2941, 0.7939, 0.3571, 0.2405, 0.6296]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 5, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:17.245038Z",
     "iopub.status.busy": "2025-02-24T04:05:17.244829Z",
     "iopub.status.idle": "2025-02-24T04:05:17.248989Z",
     "shell.execute_reply": "2025-02-24T04:05:17.248525Z",
     "shell.execute_reply.started": "2025-02-24T04:05:17.245022Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7623, 0.1894, 0.7829, 0.0639, 0.0454],\n",
       "        [0.5192, 0.9582, 0.2377, 0.4569, 0.7393],\n",
       "        [0.9676, 0.7729, 0.2908, 0.3283, 0.3356],\n",
       "        [0.4077, 0.2330, 0.7772, 0.2610, 0.9602],\n",
       "        [0.2165, 0.5694, 0.4174, 0.0278, 0.8129]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 5, requires_grad=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch会自动追踪和记录对与张量的所有操作，当计算完成后调用.backward()方法自动计算梯度并且将计算结果保存到grad属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:17.655357Z",
     "iopub.status.busy": "2025-02-24T04:05:17.655005Z",
     "iopub.status.idle": "2025-02-24T04:05:17.659125Z",
     "shell.execute_reply": "2025-02-24T04:05:17.658739Z",
     "shell.execute_reply.started": "2025-02-24T04:05:17.655342Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.0674, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=torch.sum(x+y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在张量进行操作后，grad_fn已经被赋予了一个新的函数，这个函数引用了一个创建了这个Tensor类的Function对象。\n",
    "Tensor和Function互相连接生成了一个非循环图，它记录并且编码了完整的计算历史。每个张量都有一个.grad_fn属性，如果这个张量是用户手动创建的那么这个张量的grad_fn是None。\n",
    "\n",
    "下面我们来调用反向传播函数，计算其梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单的自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:18.256224Z",
     "iopub.status.busy": "2025-02-24T04:05:18.255812Z",
     "iopub.status.idle": "2025-02-24T04:05:18.266616Z",
     "shell.execute_reply": "2025-02-24T04:05:18.266204Z",
     "shell.execute_reply.started": "2025-02-24T04:05:18.256207Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad,y.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果Tensor类表示的是一个标量（即它包含一个元素的张量），则不需要为backward()指定任何参数，但是如果它有更多的元素，则需要指定一个gradient参数，它是形状匹配的张量。\n",
    "以上的 `z.backward()`相当于是`z.backward(torch.tensor(1.))`的简写。\n",
    "这种参数常出现在图像分类中的单标签分类，输出一个标量代表图像的标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复杂的自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:21.742372Z",
     "iopub.status.busy": "2025-02-24T04:05:21.742061Z",
     "iopub.status.idle": "2025-02-24T04:05:21.748798Z",
     "shell.execute_reply": "2025-02-24T04:05:21.748402Z",
     "shell.execute_reply.started": "2025-02-24T04:05:21.742354Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8759, 0.8360, 0.1714, 0.7811, 0.0265],\n",
       "        [0.3532, 0.1657, 0.0772, 1.3617, 0.1615],\n",
       "        [1.5500, 0.2219, 0.6496, 1.2906, 1.1732],\n",
       "        [0.0632, 0.9383, 0.8841, 0.0333, 0.0028],\n",
       "        [0.0514, 0.9651, 0.4184, 0.4937, 0.2665]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 5, requires_grad=True)\n",
    "y = torch.rand(5, 5, requires_grad=True)\n",
    "z= x**2+y**3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:21.948903Z",
     "iopub.status.busy": "2025-02-24T04:05:21.948502Z",
     "iopub.status.idle": "2025-02-24T04:05:22.522124Z",
     "shell.execute_reply": "2025-02-24T04:05:22.521657Z",
     "shell.execute_reply.started": "2025-02-24T04:05:21.948886Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6052, 0.6341, 0.7569, 1.7657, 0.3058],\n",
      "        [0.3197, 0.8039, 0.5413, 1.7047, 0.5634],\n",
      "        [1.6639, 0.6602, 1.0162, 1.9732, 1.7399],\n",
      "        [0.1609, 1.1020, 1.8803, 0.3610, 0.1062],\n",
      "        [0.4449, 1.4518, 1.2935, 1.3111, 0.9762]])\n"
     ]
    }
   ],
   "source": [
    "#我们的返回值不是一个标量，所以需要输入一个大小相同的张量作为参数，这里我们用ones_like函数根据x生成一个张量\n",
    "z.backward(torch.ones_like(x))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用with torch.no_grad()上下文管理器临时禁止对已设置requires_grad=True的张量进行自动求导。这个方法在测试集计算准确率的时候会经常用到，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:22.523293Z",
     "iopub.status.busy": "2025-02-24T04:05:22.522935Z",
     "iopub.status.idle": "2025-02-24T04:05:22.526392Z",
     "shell.execute_reply": "2025-02-24T04:05:22.526009Z",
     "shell.execute_reply.started": "2025-02-24T04:05:22.523275Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((x +y*2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用.no_grad()进行嵌套后，代码不会跟踪历史记录，也就是说保存的这部分记录会减少内存的使用量并且会加快少许的运算速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络包nn和优化器optm\n",
    "torch.nn是专门为神经网络设计的模块化接口。nn构建于 Autograd之上，可用来定义和运行神经网络。\n",
    "这里我们主要介绍几个一些常用的类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**约定：torch.nn 我们为了方便使用，会为他设置别名为nn，本章除nn以外还有其他的命名约定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:24.209983Z",
     "iopub.status.busy": "2025-02-24T04:05:24.209655Z",
     "iopub.status.idle": "2025-02-24T04:05:24.213309Z",
     "shell.execute_reply": "2025-02-24T04:05:24.212813Z",
     "shell.execute_reply.started": "2025-02-24T04:05:24.209963Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu121'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先要引入相关的包\n",
    "import torch\n",
    "# 引入torch.nn并指定别名\n",
    "import torch.nn as nn\n",
    "#打印一下版本\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了nn别名以外，我们还引用了nn.functional，这个包中包含了神经网络中使用的一些常用函数，这些函数的特点是，不具有可学习的参数(如ReLU，pool，DropOut等)，这些函数可以放在构造函数中，也可以不放，但是这里建议不放。\n",
    "\n",
    "一般情况下我们会**将nn.functional 设置为大写的F**，这样缩写方便调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:25.087266Z",
     "iopub.status.busy": "2025-02-24T04:05:25.086969Z",
     "iopub.status.idle": "2025-02-24T04:05:25.089650Z",
     "shell.execute_reply": "2025-02-24T04:05:25.089236Z",
     "shell.execute_reply.started": "2025-02-24T04:05:25.087248Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义一个网络\n",
    "PyTorch中已经为我们准备好了现成的网络模型，只要继承nn.Module，并实现它的forward方法，PyTorch会根据autograd，自动实现backward函数，在forward函数中可使用任何tensor支持的函数，还可以使用if、for循环、print、log等Python语法，写法和标准的Python写法一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:29.011563Z",
     "iopub.status.busy": "2025-02-24T04:05:29.011254Z",
     "iopub.status.idle": "2025-02-24T04:05:29.017414Z",
     "shell.execute_reply": "2025-02-24T04:05:29.016928Z",
     "shell.execute_reply.started": "2025-02-24T04:05:29.011544Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1350, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # nn.Module子类的函数必须在构造函数中执行父类的构造函数\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 卷积层 '1'表示输入图片为单通道， '6'表示输出通道数，'3'表示卷积核为3*3\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3) \n",
    "        #线性层，输入1350个特征，输出10个特征\n",
    "        self.fc1   = nn.Linear(1350, 10)  #这里的1350是如何计算的呢？这就要看后面的forward函数\n",
    "    #正向传播 \n",
    "    def forward(self, x): \n",
    "        print(x.size()) # 结果：[1, 1, 32, 32]\n",
    "        # 卷积 -> 激活 -> 池化 \n",
    "        x = self.conv1(x) #根据卷积的尺寸计算公式，计算结果是30，具体计算公式后面第二章第四节 卷积神经网络 有详细介绍。\n",
    "        x = F.relu(x)\n",
    "        print(x.size()) # 结果：[1, 6, 30, 30]\n",
    "        x = F.max_pool2d(x, (2, 2)) #我们使用池化层，计算结果是15\n",
    "        x = F.relu(x)\n",
    "        print(x.size()) # 结果：[1, 6, 15, 15]\n",
    "        # reshape，‘-1’表示自适应\n",
    "        #这里做的就是压扁的操作 就是把后面的[1, 6, 15, 15]压扁，变为 [1, 1350]\n",
    "        x = x.view(x.size()[0], -1) \n",
    "        print(x.size()) # 这里就是fc1层的的输入1350 \n",
    "        x = self.fc1(x)        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络的可学习参数通过net.parameters()返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:30.151090Z",
     "iopub.status.busy": "2025-02-24T04:05:30.150780Z",
     "iopub.status.idle": "2025-02-24T04:05:30.161794Z",
     "shell.execute_reply": "2025-02-24T04:05:30.161320Z",
     "shell.execute_reply.started": "2025-02-24T04:05:30.151070Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0007, -0.3033, -0.1665],\n",
      "          [ 0.2051,  0.1831, -0.0097],\n",
      "          [ 0.1502, -0.1062, -0.0263]]],\n",
      "\n",
      "\n",
      "        [[[-0.0299, -0.1013,  0.1857],\n",
      "          [ 0.2592,  0.3131, -0.0961],\n",
      "          [ 0.1883,  0.0501, -0.1307]]],\n",
      "\n",
      "\n",
      "        [[[-0.0781, -0.3174, -0.0590],\n",
      "          [ 0.0132, -0.2880, -0.0644],\n",
      "          [-0.2720, -0.1075,  0.1677]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3266,  0.2958,  0.3015],\n",
      "          [ 0.1272,  0.1986, -0.1277],\n",
      "          [ 0.3194,  0.3263,  0.1440]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0809, -0.0110, -0.2811],\n",
      "          [-0.0859, -0.1482,  0.2395],\n",
      "          [ 0.2147, -0.0443,  0.3265]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0770,  0.1275,  0.2770],\n",
      "          [ 0.1241,  0.0377, -0.0944],\n",
      "          [-0.1414, -0.0039, -0.0945]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2325,  0.2301,  0.0603,  0.0476, -0.0230,  0.0546],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0195, -0.0057, -0.0251,  ...,  0.0091,  0.0087,  0.0015],\n",
      "        [ 0.0081,  0.0157, -0.0212,  ..., -0.0185, -0.0191,  0.0226],\n",
      "        [ 0.0110,  0.0008,  0.0222,  ..., -0.0079,  0.0141, -0.0241],\n",
      "        ...,\n",
      "        [ 0.0132,  0.0023, -0.0147,  ..., -0.0084, -0.0007, -0.0149],\n",
      "        [ 0.0146,  0.0241, -0.0207,  ...,  0.0042, -0.0127, -0.0212],\n",
      "        [-0.0019, -0.0210, -0.0090,  ...,  0.0180, -0.0082, -0.0188]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0056, -0.0183, -0.0172,  0.0102, -0.0057, -0.0156, -0.0017,  0.0124,\n",
      "         0.0238,  0.0091], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for parameters in net.parameters():\n",
    "    print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net.named_parameters可同时返回可学习的参数及名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:31.097687Z",
     "iopub.status.busy": "2025-02-24T04:05:31.097364Z",
     "iopub.status.idle": "2025-02-24T04:05:31.100998Z",
     "shell.execute_reply": "2025-02-24T04:05:31.100424Z",
     "shell.execute_reply.started": "2025-02-24T04:05:31.097666Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight : torch.Size([6, 1, 3, 3])\n",
      "conv1.bias : torch.Size([6])\n",
      "fc1.weight : torch.Size([10, 1350])\n",
      "fc1.bias : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in net.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward函数的输入和输出都是Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:31.833648Z",
     "iopub.status.busy": "2025-02-24T04:05:31.833370Z",
     "iopub.status.idle": "2025-02-24T04:05:31.851653Z",
     "shell.execute_reply": "2025-02-24T04:05:31.851091Z",
     "shell.execute_reply.started": "2025-02-24T04:05:31.833629Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 6, 30, 30])\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 1350])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32) # 这里的对应前面fforward的输入是32\n",
    "out = net(input)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:32.209288Z",
     "iopub.status.busy": "2025-02-24T04:05:32.208938Z",
     "iopub.status.idle": "2025-02-24T04:05:32.212165Z",
     "shell.execute_reply": "2025-02-24T04:05:32.211800Z",
     "shell.execute_reply.started": "2025-02-24T04:05:32.209271Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向传播前，先要将所有参数的梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:32.628082Z",
     "iopub.status.busy": "2025-02-24T04:05:32.627843Z",
     "iopub.status.idle": "2025-02-24T04:05:32.637850Z",
     "shell.execute_reply": "2025-02-24T04:05:32.637400Z",
     "shell.execute_reply.started": "2025-02-24T04:05:32.628066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.zero_grad() \n",
    "out.backward(torch.ones(1,10)) # 反向传播的实现是PyTorch自动实现的，我们只要调用这个函数即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**:torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。\n",
    "\n",
    "也就是说，就算我们输入一个样本，也会对样本进行分批，所以，所有的输入都会增加一个维度，我们对比下刚才的input，nn中定义为3维，但是我们人工创建时多增加了一个维度，变为了4维，最前面的1即为batch-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "在nn中PyTorch还预制了常用的损失函数，下面我们用MSELoss用来计算均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:34.209698Z",
     "iopub.status.busy": "2025-02-24T04:05:34.209436Z",
     "iopub.status.idle": "2025-02-24T04:05:34.216991Z",
     "shell.execute_reply": "2025-02-24T04:05:34.216596Z",
     "shell.execute_reply.started": "2025-02-24T04:05:34.209681Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.1951961517334\n"
     ]
    }
   ],
   "source": [
    "y = torch.arange(0,10).view(1,10).float()\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(out, y)\n",
    "#loss是个scalar，我们可以直接用item获取到他的python类型的数值\n",
    "print(loss.item()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器\n",
    "在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下：\n",
    "\n",
    "weight = weight - learning_rate * gradient\n",
    "\n",
    "在torch.optim中实现大多数的优化方法，例如RMSProp、Adam、SGD等，下面我们使用SGD做个简单的样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:34.816760Z",
     "iopub.status.busy": "2025-02-24T04:05:34.816458Z",
     "iopub.status.idle": "2025-02-24T04:05:34.818997Z",
     "shell.execute_reply": "2025-02-24T04:05:34.818611Z",
     "shell.execute_reply.started": "2025-02-24T04:05:34.816741Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:35.151889Z",
     "iopub.status.busy": "2025-02-24T04:05:35.151631Z",
     "iopub.status.idle": "2025-02-24T04:05:35.736160Z",
     "shell.execute_reply": "2025-02-24T04:05:35.735659Z",
     "shell.execute_reply.started": "2025-02-24T04:05:35.151871Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 6, 30, 30])\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 1350])\n"
     ]
    }
   ],
   "source": [
    "out = net(input) # 这里调用的时候会打印出我们在forword函数中打印的x的大小\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(out, y)\n",
    "#新建一个优化器，SGD只需要要调整的参数和学习率\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.01)\n",
    "# 先梯度清零(与net.zero_grad()效果一样)\n",
    "optimizer.zero_grad() \n",
    "loss.backward()\n",
    "\n",
    "#更新参数\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，神经网络的数据的一个完整的传播就已经通过PyTorch实现了，下面将介绍PyTorch提供的数据加载和处理工具，使用这些工具可以方便的处理所需要的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据的加载和预处理\n",
    "PyTorch通过torch.utils.data对一般常用的数据加载进行了封装，可以很容易地实现多线程数据预读和批量加载。\n",
    "并且torchvision已经预先实现了常用图像数据集，包括作业中使用过的CIFAR-10数据集，可通过torchvision.datasets方便的调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Dataset是一个抽象类，为了能够方便的读取，需要将要使用的数据包装为Dataset类。\n",
    "自定义的Dataset需要继承它并且实现两个成员方法：\n",
    "1. `__getitem__()` 该方法定义用索引(`0` 到 `len(self)`)获取一条数据或一个样本\n",
    "2. `__len__()` 该方法返回数据集的总长度\n",
    "\n",
    "下面我们使用kaggle上的一个竞赛[bluebook for bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers/data)自定义一个数据集，为了方便介绍，我们使用里面的数据字典来做说明（因为条数少）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:37.479019Z",
     "iopub.status.busy": "2025-02-24T04:05:37.478644Z",
     "iopub.status.idle": "2025-02-24T04:05:38.223078Z",
     "shell.execute_reply": "2025-02-24T04:05:38.222620Z",
     "shell.execute_reply.started": "2025-02-24T04:05:37.478998Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#引用\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:38.410750Z",
     "iopub.status.busy": "2025-02-24T04:05:38.410442Z",
     "iopub.status.idle": "2025-02-24T04:05:38.413960Z",
     "shell.execute_reply": "2025-02-24T04:05:38.413556Z",
     "shell.execute_reply.started": "2025-02-24T04:05:38.410733Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#定义一个数据集\n",
    "class BulldozerDataset(Dataset):\n",
    "    \"\"\" 数据集演示 \"\"\"\n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"实现初始化方法，在初始化的时候将数据读载入\"\"\"\n",
    "        self.df=pd.read_csv(csv_file)\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回df的长度\n",
    "        '''\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        根据 idx 返回一行数据\n",
    "        '''\n",
    "        return self.df.iloc[idx].SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们的数据集已经定义完成了，我们可以实例化一个对象访问它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:39.830559Z",
     "iopub.status.busy": "2025-02-24T04:05:39.830279Z",
     "iopub.status.idle": "2025-02-24T04:05:39.844997Z",
     "shell.execute_reply": "2025-02-24T04:05:39.844511Z",
     "shell.execute_reply.started": "2025-02-24T04:05:39.830542Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_demo= BulldozerDataset('median_benchmark.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以直接使用如下命令查看数据集数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:41.280383Z",
     "iopub.status.busy": "2025-02-24T04:05:41.280066Z",
     "iopub.status.idle": "2025-02-24T04:05:41.283864Z",
     "shell.execute_reply": "2025-02-24T04:05:41.283239Z",
     "shell.execute_reply.started": "2025-02-24T04:05:41.280364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11573"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#实现了 __len__ 方法所以可以直接使用len获取数据总数\n",
    "len(ds_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:42.057553Z",
     "iopub.status.busy": "2025-02-24T04:05:42.057204Z",
     "iopub.status.idle": "2025-02-24T04:05:42.063952Z",
     "shell.execute_reply": "2025-02-24T04:05:42.063358Z",
     "shell.execute_reply.started": "2025-02-24T04:05:42.057535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用索引可以直接访问对应的数据，对应 __getitem__ 方法\n",
    "ds_demo[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义的数据集已经创建好了，下面我们使用官方提供的数据载入器，读取数据\n",
    "## Dataloader\n",
    "DataLoader为我们提供了对Dataset的读取操作，常用参数有：batch_size(每个batch的大小)、 shuffle(是否进行shuffle操作)、 num_workers(加载数据的时候使用几个子进程)。下面做一个简单的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:42.502763Z",
     "iopub.status.busy": "2025-02-24T04:05:42.502377Z",
     "iopub.status.idle": "2025-02-24T04:05:42.505477Z",
     "shell.execute_reply": "2025-02-24T04:05:42.504878Z",
     "shell.execute_reply.started": "2025-02-24T04:05:42.502746Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(ds_demo, batch_size=10, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader返回的是一个可迭代对象，我们可以使用迭代器分次获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:42.954826Z",
     "iopub.status.busy": "2025-02-24T04:05:42.954521Z",
     "iopub.status.idle": "2025-02-24T04:05:42.960578Z",
     "shell.execute_reply": "2025-02-24T04:05:42.960143Z",
     "shell.execute_reply.started": "2025-02-24T04:05:42.954810Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,\n",
      "        24000.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "idata=iter(dl)\n",
    "print(next(idata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的用法是使用for循环对其进行遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:05:43.817753Z",
     "iopub.status.busy": "2025-02-24T04:05:43.817431Z",
     "iopub.status.idle": "2025-02-24T04:05:43.823125Z",
     "shell.execute_reply": "2025-02-24T04:05:43.822685Z",
     "shell.execute_reply.started": "2025-02-24T04:05:43.817737Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,\n",
      "        24000.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dl):\n",
    "    print(i,data)\n",
    "    # 为了节约空间，这里只循环一遍\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经可以通过dataset定义数据集，并使用Datalorder载入和遍历数据集，除了这些以外，PyTorch还提供能torcvision的计算机视觉扩展包，里面封装了\n",
    "## torchvision 包\n",
    "torchvision 是PyTorch中专门用来处理图像的库，PyTorch官网的安装教程中最后的pip install torchvision 就是安装这个包。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchvision.datasets\n",
    "torchvision.datasets 可以理解为PyTorch团队自定义的dataset，这些dataset帮我们提前处理好了很多的图片数据集，我们拿来就可以直接使用：\n",
    "- MNIST\n",
    "- COCO\n",
    "- Captions\n",
    "- Detection\n",
    "- LSUN\n",
    "- ImageFolder\n",
    "- Imagenet-12\n",
    "- CIFAR\n",
    "- STL10\n",
    "- SVHN\n",
    "- PhotoTour\n",
    "我们可以直接使用，示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "trainset = datasets.MNIST(root='./data', # 表示 MNIST 数据的加载的目录\n",
    "                                      train=True,  # 表示是否加载数据库的训练集，false的时候加载测试集\n",
    "                                      download=True, # 表示是否自动下载 MNIST 数据集\n",
    "                                      transform=None) # 表示是否需要对数据进行预处理，none为不进行预处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchvision.models\n",
    "torchvision不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用，或者在进行迁移学习\n",
    "torchvision.models模块的 子模块中包含以下模型结构。\n",
    "- AlexNet\n",
    "- VGG\n",
    "- ResNet\n",
    "- SqueezeNet\n",
    "- DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#我们直接可以使用训练好的模型，当然这个与datasets相同，都是需要从服务器下载的\n",
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchvision.transforms\n",
    "transforms 模块提供了一般的图像转换操作类，用作数据处理和数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T04:07:02.677448Z",
     "iopub.status.busy": "2025-02-24T04:07:02.677148Z",
     "iopub.status.idle": "2025-02-24T04:07:02.681194Z",
     "shell.execute_reply": "2025-02-24T04:07:02.680562Z",
     "shell.execute_reply.started": "2025-02-24T04:07:02.677429Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms as transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  #先四周填充0，在把图像随机裁剪成32*32\n",
    "    transforms.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转\n",
    "    transforms.RandomRotation((-45,45)), #随机旋转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.229, 0.224, 0.225)), #R,G,B每层的归一化用到的均值和方差\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "肯定有人会问：(0.485, 0.456, 0.406), (0.2023, 0.1994, 0.2010) 这几个数字是什么意思？\n",
    "\n",
    "官方的这个帖子有详细的说明:\n",
    "https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21\n",
    "这些都是根据ImageNet训练的归一化参数，可以直接使用，我们认为这个是固定值就可以"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
